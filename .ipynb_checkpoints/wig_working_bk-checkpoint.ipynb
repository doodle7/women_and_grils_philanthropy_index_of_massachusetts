{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c05e2f-8a36-4257-bb98-dec7ca0b628a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.31.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 57.6/57.6 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting xlsx2csv\n",
      "  Downloading xlsx2csv-0.8.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (1.25.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (2019.6.16)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: xlsx2csv, tqdm\n",
      "Successfully installed tqdm-4.66.1 xlsx2csv-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 requests tqdm\n",
    "xlsx2csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da6957f-4031-4bf3-9b05-0f0dfa3836a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from zipcodes import greater_boston_zipcodes\n",
    "import concurrent.futures\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from xlsx2csv import Xlsx2csv\n",
    "import requests\n",
    "\n",
    "WGI_URL = 'https://wgi.communityplatform.us/'\n",
    "WGI_API_BASE = 'https://wgi.communityplatform.us/platform-api/'\n",
    "STATE = 'MA'\n",
    "ORG_LIST_URL = f'{WGI_API_BASE}search/base-search?page=1&perPage=10000&orderBy=revenue&keywordType=all&resultType=all&states[]={STATE}'\n",
    "# https://wgi.communityplatform.us/platform-api/search/base-search?page=1&perPage=10000&orderBy=revenue&keywordType=all&resultType=all&states[]=MA\n",
    "IRS_SRC_URL = 'https://www.irs.gov/statistics/soi-tax-stats-annual-extract-of-tax-exempt-organization-financial-data'\n",
    "DEV_EMAIL = 'elijahllopezz@gmail.com'\n",
    "script_dir = Path(os.path.dirname(os.path.abspath(sys.argv[0])))\n",
    "API = 'https://wgi.communityplatform.us/platform-api/search/base-search?page=1&perPage=40&orderBy=revenue&keywordType=all&resultType=all&states%5B%5D=MA&onlyFilers=true&searchView=map'\n",
    "\n",
    "def download_file(link, filepath, force=False):\n",
    "    if not os.path.exists(filepath) or force:\n",
    "        response = requests.get(link, stream=True)\n",
    "        with open(filepath, 'wb') as f:\n",
    "            # 10MB chunk size\n",
    "            for chunk in tqdm(response.iter_content(chunk_size=10_000_000),\n",
    "                              desc=f'Downloading {filepath.name}'):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "\n",
    "def xlsx_to_csv(src, dest=None, force=False):\n",
    "    stem = src.stem\n",
    "    if dest is None:\n",
    "        dest = src.with_suffix('.csv')\n",
    "    if not os.path.exists(dest) or force:\n",
    "        print(f'Converting {stem}.xslx to {stem}.csv...')\n",
    "        Xlsx2csv(str(src), outputencoding='utf-8').convert(str(dest))\n",
    "    #print(\"Dest\\n\")\n",
    "    #print(dest)\n",
    "    return dest\n",
    "\n",
    "\n",
    "def get_download_links():\n",
    "    \"\"\"Parse download links for excel forms from IRS website\n",
    "\n",
    "    Returns:\n",
    "        dict(int:list(dict{filename, link})): list of excel download links for each year\n",
    "    \"\"\"\n",
    "    r = requests.get(IRS_SRC_URL)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    # get all <h2> that appear before tables\n",
    "    #pint(soup)\n",
    "    h2s = filter(\n",
    "        lambda tag: tag.text.startswith(\n",
    "            'Exempt Organization Returns Filed in Calendar Year'),\n",
    "        soup.find_all('h2'))\n",
    "    download_links = {}\n",
    "    for h2 in h2s:\n",
    "        year = int(re.search(r'\\d+', h2.text).group(0))\n",
    "        table = h2.find_next()\n",
    "        ##int(table)\n",
    "        links = [{\n",
    "            'name': f'{a_tag.text} ({year})',\n",
    "            'link': a_tag['href'],\n",
    "        } for a_tag in table.find_all('a')]\n",
    "        download_links[year] = links\n",
    "    #print(download_links)\n",
    "    return download_links\n",
    "\n",
    "\n",
    "def download_raw_data(year: int, force=False):\n",
    "    \"\"\"Download excel files from the IRS website corresponding to the year and convert to CSV\n",
    "    Example of exported file: `script_dir/2021/Form 990 Extract (2021).csv`\n",
    "\n",
    "    Raises RuntimeError if something went wrong\n",
    "    Raises ValueError if year is not available to download\n",
    "\n",
    "    Args:\n",
    "        year (int): year for which files to download\n",
    "    \"\"\"\n",
    "    # https://pythonprogramming.net/introduction-scraping-parsing-beautiful-soup-tutorial/\n",
    "    # https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "    try:\n",
    "        download_links = get_download_links()\n",
    "    except requests.RequestException:\n",
    "        raise RuntimeError(f'Could not access IRS website ({IRS_SRC_URL})')\n",
    "    if not download_links:\n",
    "        raise RuntimeError('Could not parse IRS website to fetch links')\n",
    "    if year not in download_links:\n",
    "        raise ValueError(f'Year {year} is unavailable')\n",
    "\n",
    "    # download files into directory\n",
    "\n",
    "    download_folder = script_dir / f'{year}'\n",
    "    os.makedirs(download_folder, exist_ok=True)\n",
    "    total_contrib = 0\n",
    "    for file in download_links[year]:\n",
    "        name = download_folder / file['name']\n",
    "        xlsx_file = name.with_suffix('.xlsx')\n",
    "        download_file(file['link'], xlsx_file, force=force)\n",
    "        csv_file = str(xlsx_to_csv(xlsx_file, force=force))\n",
    "        print(\"Conversion Done!!\")\n",
    "        if '990-EZ' in csv_file:\n",
    "            with open(csv_file) as f:\n",
    "                total_contrib += sum(int(r['totcntrbs']) for r in csv.DictReader(f))\n",
    "        elif 'Form 990 Extract' in csv_file:\n",
    "            with open(csv_file) as f:\n",
    "                total_contrib += sum(int(r['totcntrbgfts']) for r in csv.DictReader(f))\n",
    "        #comment below\n",
    "       ## print(f'{name.stem} headers: ', end='')\n",
    "       # with open(csv_file) as f:\n",
    "       #      csv_reader = csv.reader(f, delimiter = ',')\n",
    "       #      for row in csv_reader:\n",
    "       #          print(', '.join(row))\n",
    "       #          break\n",
    "    #comment above \n",
    "    print('Completed required download and CSV conversions')\n",
    "    print('Total Contribution:', total_contrib)\n",
    "    return total_contrib\n",
    "\n",
    "\n",
    "def get_latest_wgi(force=False):\n",
    "    r = requests.get(WGI_URL)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    a_tag = soup.find('a', string='Download The List')\n",
    "    if not a_tag:\n",
    "        raise RuntimeError('Could not download WGI list from ')\n",
    "    dl_link = a_tag['href']\n",
    "    wgi_dir = script_dir / 'WGI'\n",
    "    os.makedirs(wgi_dir, exist_ok=True)\n",
    "    xlsx_file = wgi_dir / Path(dl_link).name\n",
    "    download_file(dl_link, xlsx_file, force=force)\n",
    "    #print(\"Get latest Wgi done!\")\n",
    "    return xlsx_to_csv(xlsx_file)\n",
    "\n",
    "\n",
    "def get_org(org_id):\n",
    "\n",
    "    # https://wgi.communityplatform.us/platform-api/organization/1776515\n",
    "    url = f'{WGI_API_BASE}organization/{org_id}'\n",
    "    r = requests.get(url)\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "def get_gba_orgs():\n",
    "    \"\"\"\n",
    "    Get orgs from the Greater Boston Area\n",
    "    \"\"\"\n",
    "    # greater_boston_zipcodes\n",
    "    orgs = requests.get(ORG_LIST_URL).json()['data']\n",
    "    \n",
    "    print(\"Greater Boston raw data\\n\")#extra print\n",
    "    #print(orgs) #extra print\n",
    "   # print(\"\\n\") #extra print\n",
    "    \n",
    "    org_in_state = {}\n",
    "    wg_revenue = 0\n",
    "    output_file = script_dir / 'output.csv'\n",
    "    try:\n",
    "        with open(output_file) as f:\n",
    "            wg_revenue = sum(int(r['revenue']) for r in csv.DictReader(f))\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        # https://wgi.communityplatform.us/platform-api/organization/1776515\n",
    "        for org in orgs:\n",
    "            org_zip = int(org['zip'])\n",
    "            if org_zip in greater_boston_zipcodes:\n",
    "                # clean data\n",
    "                org.pop('distance')\n",
    "                org.pop('icon')\n",
    "                org.pop('programId')\n",
    "                org.pop('programName')\n",
    "                org.pop('redirectUrl')\n",
    "                org.pop('relevance')\n",
    "                org['ein'] = ''\n",
    "                org_in_state[org['organizationId']] = org\n",
    "                revenue = org['revenue']\n",
    "                #print(\"Printing Revenue \\n\")\n",
    "                if isinstance(revenue, str):\n",
    "                    revenue = revenue.strip()\n",
    "                    if revenue[0] == '(' and revenue[-1] == ')':\n",
    "                        revenue = f'-{revenue[1:-1]}'\n",
    "                    if revenue == '-':\n",
    "                        revenue = 0\n",
    "                    revenue = int(revenue)\n",
    "                org['revenue'] = revenue\n",
    "                wg_revenue += revenue\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            future_to_org = {executor.submit(get_org, org_id): org_id for org_id in org_in_state}\n",
    "            # Order is not guaranteed even if you use a list. Use the value part above as an index\n",
    "            for future in tqdm(concurrent.futures.as_completed(future_to_org), total=len(future_to_org), desc='Downloading organization data'):\n",
    "                try:\n",
    "                    org_id = future_to_org[future]\n",
    "                    res = future.result()\n",
    "                    org_in_state[org_id]['ein'] = res['ein']\n",
    "                except ValueError as e:\n",
    "                    print(e)\n",
    "\n",
    "        with open(output_file, 'w') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=list(next(iter(org_in_state.values())).keys()))\n",
    "            writer.writeheader()\n",
    "            writer.writerows(org_in_state.values())\n",
    "    print('Organization data for Greater Boston Area found in:', output_file)\n",
    "    print('Revenue W&G organizations:', wg_revenue)\n",
    "    return wg_revenue\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2cd21b-d9c9-46be-a52a-5fe8b16537ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater Boston raw data\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading organization data: 100%|█████████████████████████████████████████████████| 857/857 [00:32<00:00, 26.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organization data for Greater Boston Area found in: c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\output.csv\n",
      "Revenue W&G organizations: 774820095\n",
      "Conversion Done!!\n",
      "Conversion Done!!\n",
      "Conversion Done!!\n",
      "Conversion Done!!\n",
      "Completed required download and CSV conversions\n",
      "Total Contribution: 320298280580\n",
      "warning file not found: c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keys\\V2 April 22_WSO_GSO_MA.xlsx\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter keys source file:  Desktop/dell_Lalit/dell_Lalit/bwg/V2_April_22_WSO_GSO_MA.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning file not found: Desktop\\dell_Lalit\\dell_Lalit\\bwg\\V2_April_22_WSO_GSO_MA.xlsx\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        wg_revenue = get_gba_orgs()\n",
    "        wgi_latest = get_latest_wgi()\n",
    "        total_revenue = download_raw_data(2021)\n",
    "        #xlsx_key_list = script_dir / 'keys' / 'V2 April 22_WSO_GSO_MA.xlsx'\n",
    "        #while not os.path.exists(xlsx_key_list):\n",
    "        #    print(f'warning file not found: {xlsx_key_list}')\n",
    "        #    xlsx_key_list = Path(input('enter keys source file: '))\n",
    "        #csv_key_list = xlsx_key_list.with_suffix('.csv')\n",
    "        #if not os.path.exists(csv_key_list):\n",
    "        #    print('Converting', xlsx_key_list.name, 'to csv')\n",
    "        #    Xlsx2csv(str(xlsx_key_list),\n",
    "         #            outputencoding='utf-8').convert(str(csv_key_list))\n",
    "        print('Percent contribution:', round(wg_revenue / total_revenue * 100, 2), '%')\n",
    "    except Exception as e:\n",
    "        exc_type, exc_tb = sys.exc_info()[0], sys.exc_info()[2]\n",
    "        print(e.__repr__())\n",
    "        print(f'\\nThe error above was encountered on line {exc_tb.tb_lineno}. Please contact Elijah Lopez <{DEV_EMAIL}>')\n",
    "\n",
    "# UofM list of orgs in the greater boston area serving women & girls for 2018\n",
    "# use zip codes provided in IRS data set to find all orgs in the greater boston area\n",
    "# use this list to find contributions for greater boston orgs for years 2018-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56b9a327-22ff-4673-8f25-026bc4925998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      EIN                                              NAME  \\\n",
      "0   19818                      PALMER SECOND BAPTIST CHURCH   \n",
      "1   29215                               ST GEORGE CATHEDRAL   \n",
      "2  587764                              IGLESIA BETHESDA INC   \n",
      "3  635913  MINISTERIO APOSTOLICO JESUCRISTO ES EL SENOR INC   \n",
      "4  765634                        MERCY CHAPEL INTERNATIONAL   \n",
      "\n",
      "                         STREET          CITY STATE ZIP_PART_1 ZIP_PART_2  \n",
      "0             1050 THORNDIKE ST        PALMER    MA      01069       1507  \n",
      "1                523 E BROADWAY  SOUTH BOSTON    MA      02127       4415  \n",
      "2              13 CUMMINGHAM ST        LOWELL    MA      01852       0000  \n",
      "3                  454 ESSEX ST      LAWRENCE    MA      01840       1242  \n",
      "4  75 MORTON VILLAGE DR APT 408      MATTAPAN    MA      02126       2433  \n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas\n",
    "#!pip install openpyxl\n",
    "\n",
    "###########This is for MA ####################\n",
    "#Read eo_ma file, extract EIN, Name, City Zip (Zip needs to be seprated to) \n",
    "#Create a CSV file with this file \n",
    "#Create a list data structure with EIN ( Name, City, Zip)\n",
    "#Compare the EIN and with IRS data base both 900 and 900EZ file and update the above list with, \"totcntrbs\" in 900 and ezfile \"totcntrbgfts\"\n",
    "#Generate a CSV file with this info\n",
    "\n",
    "import pandas as pd\n",
    "file_name = 'eo_ma.csv' \n",
    "columns_to_capture = ['EIN','NAME','STREET', 'CITY', 'STATE', 'ZIP']\n",
    "df = pd.read_csv(file_name, usecols=columns_to_capture)\n",
    "df[['ZIP_PART_1', 'ZIP_PART_2']] = df['ZIP'].str.split('-', expand=True)\n",
    "df = df.drop('ZIP', axis=1)\n",
    "print(df.head())\n",
    "#write in csv file\n",
    "#df.to_csv('MA_list.csv')\n",
    "\n",
    "\n",
    "#from openpyxl import load_workbook\n",
    "\n",
    "#wb = load_workbook(filename='22eoextract990.xlsx', read_only=True)\n",
    "#ws = wb['EIN']\n",
    "\n",
    "#for row in ws.rows:\n",
    "#    for cell in row:\n",
    "#        print(cell.value)\n",
    "\n",
    "# Close the workbook after reading\n",
    "#wb.close()\n",
    "\n",
    "\n",
    "#columns_to_capture_990 = ['EIN','totcntrbs']\n",
    "#IRS_file_990_extract_file_name = '22eoextract990.xlsx'\n",
    "#df2 = pd.read_excel(IRS_file_990_extract_file_name, usecols=columns_to_capture_990,sheet_name='Sheet1', engine='xlrd')\n",
    "#print(df2.head())\n",
    "\n",
    "\n",
    "#IRS_file_990-ez_extract_file = '22eoextractez.xlsx'\n",
    "\n",
    "#df.to_csv('MA_list.csv', index=columns_to_capture)\n",
    "#print(df.head()) # print the first 5 rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d98514-34b8-4136-8f72-871c59e4b094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from zipcodes import greater_boston_zipcodes\n",
    "import concurrent.futures\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from xlsx2csv import Xlsx2csv\n",
    "import requests\n",
    "\n",
    "#!pip install xlrd\n",
    "IRS_file_990_extract_file_name = '22eoextract990.xlsx'\n",
    "import os\n",
    "def xlsx_to_csv(src, dest=None, force=False):\n",
    "    stem = str(src)\n",
    "    if dest is None:\n",
    "        dest = src+'.csv'\n",
    "    if not os.path.exists(dest) or force:\n",
    "        print(f'Converting {stem}.xslx to {stem}.csv...')\n",
    "        Xlsx2csv(str(src), outputencoding='utf-8').convert(str(dest))\n",
    "    #print(\"Dest\\n\")\n",
    "    print(dest)\n",
    "    return dest\n",
    "# get the current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "#script_dir = Path(os.path.dirname(os.path.abspath(sys.argv[0])))\n",
    "#print(script_dir)\n",
    "#file_name = xlsx_to_csv(IRS_file_990_extract_file_name, force=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "columns_to_capture_990 = ['EIN', 'totcntrbgfts']\n",
    "IRS_file_990_extract_file_name = '22eoextract990.xlsx.csv'\n",
    "df1 = pd.read_csv(IRS_file_990_extract_file_name, usecols=columns_to_capture_990)\n",
    "#df.head()\n",
    "\n",
    "file_name = 'MA_list.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "df3= pd.merge(df, df1, on='EIN', how='left')\n",
    "\n",
    "IRS_file_990_ez_extract_ez_file = '22eoextractez.xlsx'\n",
    "#script_dir = Path(os.path.dirname(os.path.abspath(sys.argv[0])))\n",
    "#print(script_dir)\n",
    "#file_name = xlsx_to_csv(IRS_file_990_ez_extract_ez_file, force=True)\n",
    "columns_to_capture_990_ez = ['ein','totcntrbs']\n",
    "IRS_file_990_extract_ez_file= '22eoextractez.xlsx.csv'\n",
    "df4 = pd.read_csv(IRS_file_990_extract_ez_file, usecols=columns_to_capture_990_ez)\n",
    "df4.rename(columns={'ein':'EIN'},inplace=True)\n",
    "\n",
    "df3= pd.merge(df3, df4, on='EIN', how='left')\n",
    "# Here you will get the combined list for contibution in MA \n",
    "df3.to_csv('updated2.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df3.head()\n",
    "\n",
    "#df4.head()\n",
    "#df = pd.read_csv(file_name, usecols=columns_to_capture)\n",
    "#import pandas as pd\n",
    "#file_name = 'eo_ma.csv' \n",
    "#columns_to_capture = ['EIN','NAME','STREET', 'CITY', 'STATE', 'ZIP']\n",
    "#df = pd.read_csv(file_name, usecols=columns_to_capture)\n",
    "#df[['ZIP_PART_1', 'ZIP_PART_2']] = df['ZIP'].str.split('-', expand=True)\n",
    "#df = df.drop('ZIP', axis=1)\n",
    "#print(df.head())\n",
    "\n",
    "#IRS_file_990-ez_extract_file = '22eoextractez.xlsx'\n",
    "#columns_to_capture_990 = ['ein','totcntrbs']\n",
    "#IRS_file_990_extract_ez_file= '22eoextract990ez.xlsx.csv'\n",
    "#df = pd.read_csv(IRS_file_990_extract_ez_file, usecols=columns_to_capture_990)\n",
    "#df.head()\n",
    "#df.to_csv('MA_list.csv', index=columns_to_capture)\n",
    "#print(df.head()) # print the first 5 rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc23a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#from zipcodes import greater_boston_zipcodes\n",
    "#zipcode = pd.read_excel('Index_Zip_Codes.xlsx', header=None)\n",
    "#numbers_list = zipcode.values.flatten().dropna().tolist()\n",
    "#print(numbers_list)\n",
    "\n",
    "#clean up new list of zip code\n",
    "#numbers_list = zipcode.values.flatten()\n",
    "#numbers_list = pd.Series(numbers_list).dropna().tolist()\n",
    "#print(numbers_list[8:])\n",
    "\n",
    "#zipcode.to_csv('greater_boston_zip.csv')\n",
    "\n",
    "\n",
    "###########This is for Greater Boston Area####################\n",
    "# Create a list with EIN greater boston  \n",
    "\n",
    "# Convert index file in csv if needed\n",
    "# Read the above MA file compare it with index zip file if the zip is there in MA fill \n",
    "# Update the List with filtered info\n",
    "# Create a CSV file  with the filtered info\n",
    "import pandas as pd\n",
    "df5 = pd.read_csv('output.csv')\n",
    "df5.rename(columns={'ein':'EIN'},inplace=True)\n",
    "df5.head()\n",
    "df5= pd.merge(df5, df1, on='EIN', how='left')\n",
    "df5= pd.merge(df5, df4, on='EIN', how='left')\n",
    "df5.to_csv('output_greater_boston.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9022f32-8b05-4a43-a274-8d4c29884eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting V2_April_22_WSO_GSO_MA.xlsx.xslx to V2_April_22_WSO_GSO_MA.xlsx.csv...\n",
      "V2_April_22_WSO_GSO_MA.xlsx.csv\n",
      "Converting WGI_MA_Only _11_6_23.xlsx.xslx to WGI_MA_Only _11_6_23.xlsx.csv...\n",
      "WGI_MA_Only _11_6_23.xlsx.csv\n"
     ]
    }
   ],
   "source": [
    "###########This is for Womens only in MA ####################\n",
    "# Create a list with EIN Womens only  MA\n",
    "# Read the above MA file compare it with Given V2 April_WSO_GSO_MA.xlsx if the zip is there in MA fill \n",
    "# Update the List with filtered info\n",
    "# Create a CSV file  with the filtered info\n",
    "V2_April_22_WSO_GSO_MA = 'V2_April_22_WSO_GSO_MA.xlsx' \n",
    "WGI_MA_Only_11_6_23 = 'WGI_MA_Only _11_6_23.xlsx'\n",
    "#script_dir = Path(os.path.dirname(os.path.abspath(sys.argv[0])))\n",
    "#print(script_dir)\n",
    "file_name = xlsx_to_csv(V2_April_22_WSO_GSO_MA, force=True)\n",
    "file_name = xlsx_to_csv(WGI_MA_Only_11_6_23, force=True)\n",
    "\n",
    "df6 =pd.read_csv('WGI_MA_Only _11_6_23.xlsx.csv')\n",
    "\n",
    "columns_to_capture_990 = ['EIN', 'totcntrbgfts']\n",
    "IRS_file_990_extract_file_name = '22eoextract990.xlsx.csv'\n",
    "df5 = pd.read_csv(IRS_file_990_extract_file_name, usecols=columns_to_capture_990)\n",
    "#df.head()\n",
    "\n",
    "\n",
    "df6= pd.merge(df6, df5, on='EIN', how='left')\n",
    "\n",
    "columns_to_capture_990_ez = ['ein','totcntrbs']\n",
    "IRS_file_990_extract_ez_file= '22eoextractez.xlsx.csv'\n",
    "df4 = pd.read_csv(IRS_file_990_extract_ez_file, usecols=columns_to_capture_990_ez)\n",
    "df4.rename(columns={'ein':'EIN'},inplace=True)\n",
    "\n",
    "df6= pd.merge(df6, df4, on='EIN', how='left')\n",
    "df6.head(10)\n",
    "df6.to_csv(\"wig_ma_only.csv\")\n",
    "# Here you will get the combined list for contibution in MA \n",
    "#df3.to_csv('updated2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7ba03-7050-485f-9098-8cc7091b4bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
